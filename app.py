# app.py
import streamlit as st
from handlers import create_chatbox_handler
from bigdl.llm.langchain.llms import TransformersLLM
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain.schema import SystemMessage  


import re
import time

st.set_page_config(
    page_title="Infinit FDU Chatbot",
    page_icon="ğŸ‘‹",
)

st.title("Infinit FDU Chatbot")

# TODO:
#  1. Let user input a custom instruction
#  2. Store this instruction in a local txt file
#  3. 

# Use cache to load model, no need to reload after web rerun
@st.cache_resource
def load_transformers_llm(model_name, max_new_tokens):
    # Define the base folder path
    base_folder_path = "F:/Study/Code/llm-models"

    # Append MODEL_NAME to the folder path
    model_path = base_folder_path + "/" + model_name

    if (model_name == "Baichuan-13B-Chat"):
        llm = TransformersLLM.from_model_id_low_bit(
            model_id=model_path,
            model_kwargs={"temperature": 0.2, "trust_remote_code": True},
            
        )

    return llm


# config sidebar
with st.sidebar:
    MODEL_NAME = st.selectbox('Choose local model',
        ("Baichuan-13B-Chat", ""),
        placeholder="Select...",)
    st.write("Model name:", MODEL_NAME)
    max_new_tokens = st.number_input("Set max new tokens", value=512)
    st.write("Max new tokens:", max_new_tokens)
    user_input_type = st.radio("Select input type", ["fact", "code", "entertainment", "utility", "personal", "default"], index=None)


# load llm
llm = load_transformers_llm(MODEL_NAME, max_new_tokens)
# set handler
# handler = create_chatbox_handler(user_input_type, llm)


if "first_deployment" not in st.session_state:
    st.session_state.first_deployment = True

if "custom_instruction" not in st.session_state:
    st.session_state.custom_instruction = ""


with st.form("IST_FORM"):
    st.info("User instructions can only apply to new chats!", icon="ğŸš¨")
    INSTRUCTION = st.text_area("Customize instructions :sunglasses:", max_chars=500,
                               )
    st.session_state.custom_instruction = INSTRUCTION
    submitted = st.form_submit_button("Submit")
if submitted:
    st.toast('Your instruction was saved!', icon='ğŸ˜')
st.session_state.first_deployment = False

# chatbox memory
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

prompt = ChatPromptTemplate.from_template(
   """
    {type_instruction}

    ç”¨æˆ·è¾“å…¥ï¼š
    {user_input}

    ä¿®æ”¹çš„è¾“å…¥å¦‚ä¸‹ï¼š
    ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼Œæˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼š
   """
)

system_message = SystemMessage(content=st.session_state.custom_instruction)
overall_prompt = (system_message + prompt)
chain = LLMChain(llm=llm, prompt=overall_prompt)

code_instruction = """æ‚¨æ˜¯ä¸€ä¸ªä»£ç ä¼˜åŒ–AIåŠ©æ‰‹ã€‚
        è¯·ä¿®æ”¹å¹¶ä¼˜åŒ–ä»¥ä¸‹ä»£ç é—®é¢˜ï¼Œä»¥ä½¿å…¶æ›´æ˜ç¡®å’Œæ¸…æ™°ã€‚è¯·ç¡®ä¿æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œè¯¦ç»†æè¿°ï¼Œä»¥ä¾¿æ›´å¥½åœ°å›ç­”é—®é¢˜ã€‚å¦‚æœä½ çš„ç­”æ¡ˆåŒ…æ‹¬ä»£ç ï¼Œç”¨markdownæ ¼å¼å±•ç¤ºä»£ç éƒ¨åˆ†ã€‚
"""
default_instruction = """æ‚¨æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹æç¤ºè¯æ’°å†™ä¸“å®¶ï¼Œä½ çš„å·¥ä½œæ˜¯ä¸ºç”¨æˆ·ä¼˜åŒ–ä»–ä»¬çš„é—®é¢˜ï¼Œè¾“å‡ºæ›´å¥½çš„é—®é¢˜ï¼Œè€Œéå›ç­”é—®é¢˜ï¼Œä½ ä¿®æ”¹åçš„ç­”æ¡ˆå°†æä¾›ç»™æ›´å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œç”±ä»–ä»¬æ¥ç»™å‡ºé—®é¢˜çš„ç­”æ¡ˆã€‚
        æ¥ä¸‹æ¥ï¼Œç”¨æˆ·ä¼šå‘ä½ æä¾›ä¸€ä¸ªé—®é¢˜ï¼Œä½ éœ€è¦å°†é—®é¢˜ä¿®æ”¹ä¸ºæ›´å¥½çš„é—®é¢˜è¾“å‡ºï¼Œè¯·æ³¨æ„è¾“å‡ºçš„é—®é¢˜ä¸é’ˆå¯¹ç”¨æˆ·ï¼Œä¸æ˜¯å¯¹äºç”¨æˆ·éœ€æ±‚çš„å†åº¦ç¡®è®¤ï¼Œè€Œç«™åœ¨ç”¨æˆ·è§†è§’å¯ä»¥ç”¨äºå‘æ›´å¼ºå¤§çš„è¯­è¨€æ¨¡å‹æé—®ã€‚
        æ¢å¥è¯è¯´ï¼Œä½ è¦æ›¿ç”¨æˆ·é—®å‡ºæ›´å¥½çš„é—®é¢˜ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œä½ éœ€è¦è®©é—®é¢˜æ›´åŠ æ˜ç¡®ã€æœ€å¥½èƒ½æä¾›ä¾‹å­è¯´æ˜ã€æ”¹æ­£é—®é¢˜ä¸­çš„è¯­æ³•é”™è¯¯ä»¥åŠäº‹å®æ€§é”™è¯¯ã€‚

        ä½ çš„å›ç­”å°†ä»¥$ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼Œæˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼š$å¼€å¤´ã€‚è¯·ä»¥é—®å·ä½œä¸ºä½ è¾“å‡ºçš„ç»“å°¾ã€‚\n

        ä¾‹å­ï¼š\né—®é¢˜è¾“å…¥$å¤æ—¦ å“²å­¦ç³»$\né—®é¢˜è¾“å‡º$ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼æˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼šå¤æ—¦å“²å­¦ç³»ä¸“ä¸šå®åŠ›æ€ä¹ˆæ ·ï¼Ÿè¯·ä¸ºæˆ‘ä»‹ç»å…¶æ¦‚æ‹¬ã€‚$\né—®é¢˜è¾“å…¥$é•¿åŸå†å“ªé‡Œ$\né—®é¢˜è¾“å‡º$ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼æˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼šé•¿åŸåœ¨å“ªé‡Œ$\né—®é¢˜è¾“å…¥$containerå¤æ•°$\né—®é¢˜è¾“å‡º$ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼æˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼šè‹±è¯­å•è¯containerçš„å¤æ•°å½¢å¼æ˜¯ä»€ä¹ˆ$"""

if user_input := st.chat_input("What is up?"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    with st.chat_message("assistant"):
        with st.spinner("Wait for it..."):
            # response = handler.handle_input(user_input)
            response = chain.predict(type_instruction = default_instruction, user_input=user_input)
        st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})