# app.py
import streamlit as st
from handlers import create_chatbox_handler
from bigdl.llm.langchain.llms import TransformersLLM
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain.schema import SystemMessage  


import re
import time

st.set_page_config(
    page_title="Infinit FDU Chatbot",
    page_icon="ğŸ‘‹",
)

st.title("Infinit FDU Chatbot")

# TODO:
#  1. Let user input a custom instruction
#  2. Store this instruction in a local txt file
#  3. 

# Use cache to load model, no need to reload after web rerun
@st.cache_resource
def load_transformers_llm(model_name):
    # Define the base folder path
    base_folder_path = "F:/Study/Code/llm-models"

    # Append MODEL_NAME to the folder path
    model_path = base_folder_path + "/" + model_name

    if (model_name == "Baichuan-13B-Chat"):
        llm = TransformersLLM.from_model_id_low_bit(
            model_id=model_path,
            model_kwargs={"temperature": 0.2, "trust_remote_code": True},
            
        )

    return llm


# config sidebar
with st.sidebar:
    model_name = st.selectbox('Choose local model',
        ("Baichuan-13B-Chat", ""),
        placeholder="Select...",)
    st.write("Model name:", model_name)
    user_input_type = st.radio("Select input type", ["multi-step", "code", "default"], index=None)

    with st.form("Customize instruction"):
        custom_instruction = st.text_area("Customize instructions :sunglasses:", max_chars=500,
                                   )
        st.session_state.custom_instruction = custom_instruction
        submitted = st.form_submit_button("Submit")
    if submitted:
        with open("system_message.txt", "w") as file:
            # Write the system message to the file
            file.write(custom_instruction)
        st.toast('Your instruction was saved!')


# load llm
llm = load_transformers_llm(model_name)
# set handler
# handler = create_chatbox_handler(user_input_type, llm)
st.success("Model " + model_name + " loaded, enjoy your journey")

if "first_deployment" not in st.session_state:
    st.session_state.first_deployment = True

if "custom_instruction" not in st.session_state:
    st.session_state.custom_instruction = ""


# chatbox memory
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

init_prompt = ChatPromptTemplate.from_template(
   """
    {type_instruction}

    ç”¨æˆ·è¾“å…¥ï¼š
    {user_input}

    ä¿®æ”¹çš„è¾“å…¥å¦‚ä¸‹ï¼š
    ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼Œæˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼š
   """
)

system_message = SystemMessage(content=st.session_state.custom_instruction)
final_prompt = (system_message + init_prompt)
chain = LLMChain(llm=llm, prompt=final_prompt)

# ======================================================== #
# ================= Generate Instruction ================= #
# ======================================================== #

multi_step_instruction = ""

code_instruction = """æ‚¨æ˜¯ä¸€ä¸ªä»£ç ä¼˜åŒ–AIåŠ©æ‰‹ã€‚
        è¯·ä¿®æ”¹å¹¶ä¼˜åŒ–ä»¥ä¸‹ä»£ç é—®é¢˜ï¼Œä»¥ä½¿å…¶æ›´æ˜ç¡®å’Œæ¸…æ™°ã€‚è¯·ç¡®ä¿æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œè¯¦ç»†æè¿°ï¼Œä»¥ä¾¿æ›´å¥½åœ°å›ç­”é—®é¢˜ã€‚å¦‚æœä½ çš„ç­”æ¡ˆåŒ…æ‹¬ä»£ç ï¼Œç”¨markdownæ ¼å¼å±•ç¤ºä»£ç éƒ¨åˆ†ã€‚
"""

judge_instruction = """ä½ æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹æç¤ºè¯è¯„åˆ†å¤§å¸ˆï¼Œç”¨æˆ·å°†è¾“å…¥ä¸€ä¸ªæç¤ºè¯ï¼Œä½ é’ˆå¯¹è¯¥é—®é¢˜ç»™å‡ºä½ çš„è¯„åˆ†ä»¥åŠä½ çš„å»ºè®®ï¼Œè¯„åˆ†ä¸º0-10ã€‚

    è¯„åˆ†ä¾æ®ä¸€ä¸‹å‡ ç‚¹ï¼š
    1. è¯¥é—®é¢˜æ˜¯å¦æœ‰é€»è¾‘è°¬è¯¯ã€æˆ–è€…æ˜¯æ­§ä¹‰ã€‚
    2. è¯¥é—®é¢˜æ˜¯å¦æœ‰è¯­æ³•é”™è¯¯ã€é”™åˆ«å­—ã€‚
    3. è¯¥é—®é¢˜æ˜¯å¦è¶³å¤Ÿå…·ä½“ï¼Œèƒ½å¸®åŠ©è¯­è¨€æ¨¡å‹ç†è§£ä»–æ‰€è¡¨è¾¾çš„æ„æ€ï¼Œå¤ªç®€çŸ­æˆ–è€…å®½æ³›çš„é—®é¢˜æ˜¯ç³Ÿç³•çš„ã€‚
    4. è¯¥é—®é¢˜æ˜¯å¦è¢«æ‹†åˆ†ä¸ºäº†å¤šä¸ªå°é—®é¢˜ï¼Œæ¿€å‘è¯­è¨€æ¨¡å‹åˆ†æ­¥éª¤æ€è€ƒçš„èƒ½åŠ›ï¼Œä»¥ä¾¿è¯­è¨€æ¨¡å‹ä¸€æ­¥æ­¥è§£å†³å®ƒã€‚
    5. è¯¥é—®é¢˜æ˜¯å¦æä¾›äº†ä¾‹å­ï¼Œä¾¿äºè¯­è¨€æ¨¡å‹ç†è§£å…·ä½“æƒ…æ™¯ã€‚

    è¯„åˆ†çš„å°ºåº¦æ˜¯ï¼š
    10åˆ†-è¶³å¤Ÿå®Œç¾ï¼Œå¯ä»¥å‘å¤§è¯­è¨€æ¨¡å‹æé—®ã€‚
    8åˆ†-åŸºæœ¬å®Œç¾ï¼Œä½†æ²¡æœ‰æä¾›ä¾‹å­æˆ–æ˜¯æ²¡æœ‰æ‹†åˆ†ä¸ºå¤šä¸ªå°é—®é¢˜ã€‚
    6åˆ†-æ²¡æœ‰æ˜æ˜¾çš„é€»è¾‘ã€è¯­æ³•é”™è¯¯ï¼Œä½†æ˜¯è¿˜æœ‰æ”¹è¿›çš„ç©ºé—´ã€‚
    4åˆ†-é—®é¢˜è¾ƒå¤§ã€‚
    2åˆ†-éœ€è¦é‡å†™ã€‚

    è¯·èµ–ä½ çš„ä¸“ä¸šçŸ¥è¯†ï¼Œç»™å‡ºä½ çš„è¯„åˆ†ï¼Œè¯·ä¸è¦ç»™å‡ºè¿‡é«˜çš„è¯„åˆ†ï¼Œä½ éœ€è¦ä¸¥æ ¼ä¸€äº›ï¼Œå°†è¯„åˆ†æ”¾åœ¨æ–¹æ‹¬å·ä¸­ï¼Œä¹‹åä½ å¯ä»¥åŠ ä¸Šå»ºè®®ã€‚ä¾‹å¦‚ï¼š[8.5]ï¼Œè¯„åˆ†èŒƒå›´ä¸º0-10åˆ†ï¼Œå°æ•°ç‚¹åä¿ç•™ä¸€ä½å°æ•°ã€‚"""

default_instruction = """æ‚¨æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹æç¤ºè¯æ’°å†™ä¸“å®¶ï¼Œä½ çš„å·¥ä½œæ˜¯ä¸ºç”¨æˆ·ä¼˜åŒ–ä»–ä»¬çš„é—®é¢˜ï¼Œè¾“å‡ºæ›´å¥½çš„é—®é¢˜ï¼Œè€Œéå›ç­”é—®é¢˜ï¼Œä½ ä¿®æ”¹åçš„ç­”æ¡ˆå°†æä¾›ç»™æ›´å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œç”±ä»–ä»¬æ¥ç»™å‡ºé—®é¢˜çš„ç­”æ¡ˆã€‚
        æ¥ä¸‹æ¥ï¼Œç”¨æˆ·ä¼šå‘ä½ æä¾›ä¸€ä¸ªé—®é¢˜ï¼Œä½ éœ€è¦å°†é—®é¢˜ä¿®æ”¹ä¸ºæ›´å¥½çš„é—®é¢˜è¾“å‡ºï¼Œè¯·æ³¨æ„è¾“å‡ºçš„é—®é¢˜ä¸é’ˆå¯¹ç”¨æˆ·ï¼Œä¸æ˜¯å¯¹äºç”¨æˆ·éœ€æ±‚çš„å†åº¦ç¡®è®¤ï¼Œè€Œç«™åœ¨ç”¨æˆ·è§†è§’å¯ä»¥ç”¨äºå‘æ›´å¼ºå¤§çš„è¯­è¨€æ¨¡å‹æé—®ã€‚
        æ¢å¥è¯è¯´ï¼Œä½ è¦æ›¿ç”¨æˆ·é—®å‡ºæ›´å¥½çš„é—®é¢˜ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œä½ éœ€è¦è®©é—®é¢˜æ›´åŠ æ˜ç¡®ã€æœ€å¥½èƒ½æä¾›ä¾‹å­è¯´æ˜ã€æ”¹æ­£é—®é¢˜ä¸­çš„è¯­æ³•é”™è¯¯ä»¥åŠäº‹å®æ€§é”™è¯¯ã€‚

        ä½ çš„å›ç­”å°†ä»¥$ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼Œæˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼š$å¼€å¤´ã€‚è¯·ä»¥é—®å·ä½œä¸ºä½ è¾“å‡ºçš„ç»“å°¾ã€‚\n

        ä¾‹å­ï¼š\né—®é¢˜è¾“å…¥$å¤æ—¦ å“²å­¦ç³»$\né—®é¢˜è¾“å‡º$ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼æˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼šå¤æ—¦å“²å­¦ç³»ä¸“ä¸šå®åŠ›æ€ä¹ˆæ ·ï¼Ÿè¯·ä¸ºæˆ‘ä»‹ç»å…¶æ¦‚æ‹¬ã€‚$\né—®é¢˜è¾“å…¥$é•¿åŸå†å“ªé‡Œ$\né—®é¢˜è¾“å‡º$ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼æˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼šé•¿åŸåœ¨å“ªé‡Œ$\né—®é¢˜è¾“å…¥$containerå¤æ•°$\né—®é¢˜è¾“å‡º$ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼æˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼šè‹±è¯­å•è¯containerçš„å¤æ•°å½¢å¼æ˜¯ä»€ä¹ˆ$"""

# ======================================================== #
# ======================================================== #
# ======================================================== #
def extract_transformed_text(response):
    # Use regex to extract text after "ä¿®æ”¹çš„è¾“å…¥å¦‚ä¸‹ï¼š"
    match = re.search(r'ä¿®æ”¹çš„è¾“å…¥å¦‚ä¸‹ï¼š(.*)', response, re.DOTALL)
    if match:
        transformed_text = match.group(1).strip()
        return transformed_text
    else:
        return "No modified question found"

if user_input := st.chat_input("What is up?"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    with st.chat_message("assistant"):
        with st.spinner("Wait for it..."):
            # response = handler.handle_input(user_input)
            if (user_input_type == "default"):
                _ = default_instruction
            elif (user_input_type == "multi-step"):
                _ = multi_step_instruction
            elif (user_input_type == "code"):
                _ = code_instruction
            elif (user_input_type == "judge"):
                _ = judge_instruction
            st.write("Final Prompt: ")
            # st.write(final_prompt.format(type_instruction = _, user_input=user_input))
            response = chain.predict(type_instruction = _, user_input=user_input)
        st.markdown(extract_transformed_text(response))
    st.session_state.messages.append({"role": "assistant", "content": response})