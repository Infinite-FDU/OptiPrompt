# app.py
import streamlit as st
from bigdl.llm.langchain.llms import TransformersLLM
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain.schema import SystemMessage

import re
import os

import logging
logging.basicConfig(format='%(asctime)s %(message)s',
                    datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.DEBUG)

# Configure the Streamlit page with custom settings
st.set_page_config(
    page_title="Infinit FDU Chatbot",
    page_icon=":ringed_planet:",
    menu_items={
        'Get help': 'https://github.com/Infinite-FDU/BigDL',
        'Report a bug': "https://github.com/Infinite-FDU/BigDL/issues",
        'About': "# Something about this app"
    }
)
st.title(":bird: Infinit FDU Chatbot")

logging.info("Streamlit page configured")


@st.cache_resource
def load_transformers_llm(model_name):
    # Define the base folder path
    base_folder_path = "F:/Study/Code/llm-models"

    # Append MODEL_NAME to the folder path
    model_path = base_folder_path + "/" + model_name

    if (model_name == "Baichuan-13B-Chat"):
        llm = TransformersLLM.from_model_id_low_bit(
            model_id=model_path,
            model_kwargs={"temperature": 0.2, "trust_remote_code": True},

        )

    return llm


if "custom_instruction" not in st.session_state:
    st.session_state.custom_instruction = ""

# Create a sidebar section in the Streamlit app for user configuration
with st.sidebar:
    # Create a selectbox to choose a local model with default options
    model_name = st.selectbox(
        'Choose local model', ("Baichuan-13B-Chat", ""), placeholder="Select...")
    st.write("Model name:", model_name)

    # Create a radio button group for selecting input types
    user_input_type = st.radio("Select input type", [
                               "multi-step", "judge", "code", "default"], index=None)

    file_path = "system_message.txt"
    # Check if the system message text file exists
    if os.path.exists(file_path):
        # If the file exists, read its contents and store it in a string
        with open(file_path, "r") as file:
            local_system_message = file.read()
    else:
        local_system_message = ""
    st.session_state.custom_instruction = local_system_message

    # Create a form for customizing instructions
    with st.form("Customize instruction"):
        custom_instruction = st.text_area(
            "Customize instructions :sunglasses:", value=local_system_message, max_chars=500)
        submitted = st.form_submit_button("Submit")

    # If the form is submitted, save the customized instruction to the system message text file
    if submitted:
        # Store the customized instruction in the session state
        st.session_state.custom_instruction = custom_instruction
        with open("system_message.txt", "w") as file:
            file.write(custom_instruction)
        st.toast('Your instruction was saved!')

logging.info("Exit side bar")

# Load the selected Transformers LLM (Language Model) based on the chosen model name
llm = load_transformers_llm(model_name)
st.success( model_name + " loaded, enjoy your journey! :laugh:")

logging.info("Transformers LLM loaded")


# Display chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


logging.info("Chat history displayed")

# ======================================================== #
# ================ Generate initial prompt =============== #
# ======================================================== #

init_prompt = ChatPromptTemplate.from_template(
    """
{type_instruction}

ç”¨æˆ·è¾“å…¥ï¼š
{user_input}

ä¿®æ”¹çš„è¾“å…¥å¦‚ä¸‹ï¼š
ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼Œæˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼š
"""
)

multi_step_init_prompt = ChatPromptTemplate.from_template(
    """
{type_instruction}

ç”¨æˆ·è¾“å…¥ï¼š
{user_input}

ä¿®æ”¹çš„è¾“å…¥å¦‚ä¸‹ï¼š
ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼ŒåŸºäºŽå·²æœ‰é—®é¢˜ï¼š{user_input}ï¼Œæˆ‘å°†é—®é¢˜æ‹†åˆ†ä¸ºå‡ æ­¥ï¼Œæ‰€ä»¥æƒ³å‘ä½ å’¨è¯¢ä¸‹åˆ—é—®é¢˜:
"""
)

# ======================================================== #
# ==================== Generate Chains =================== #
# ======================================================== #

system_message = SystemMessage(content=st.session_state.custom_instruction)
final_prompt = (system_message + init_prompt)
chain = LLMChain(llm=llm, prompt=final_prompt)

multi_step_final_prompt = (system_message + multi_step_init_prompt)
multi_step_chain = LLMChain(llm=llm, prompt=multi_step_final_prompt)

logging.info("Chains are ready")

# ======================================================== #
# ================= Generate Instructions ================ #
# ======================================================== #

multi_step_instruction = """
æ‚¨æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡åž‹æç¤ºè¯æ’°å†™ä¸“å®¶ï¼Œä½ çš„å·¥ä½œæ˜¯ä¸ºç”¨æˆ·ä¼˜åŒ–ä»–ä»¬çš„é—®é¢˜ï¼Œè¾“å‡ºæ›´å¥½çš„é—®é¢˜ï¼Œè€Œéžå›žç­”é—®é¢˜ï¼Œä½ ä¿®æ”¹åŽçš„ç­”æ¡ˆå°†æä¾›ç»™æ›´å¼ºå¤§çš„è¯­è¨€æ¨¡åž‹ï¼Œç”±ä»–ä»¬æ¥ç»™å‡ºé—®é¢˜çš„ç­”æ¡ˆã€‚
ç”¨æˆ·çš„è¾“å…¥å¯èƒ½æ¶‰åŠä¸€ä¸ªå¤æ‚çš„é—®é¢˜ï¼Œå¦‚æžœç›´æŽ¥è®©è¯­è¨€æ¨¡åž‹å›žç­”è¿™ä¸ªé—®é¢˜æ•ˆæžœå¹¶ä¸å¥½ã€‚æ‰€ä»¥è¯·ä½ æ€è€ƒï¼šå¦‚æžœè§£å†³è¿™ä¸ªé—®é¢˜å¯ä»¥åˆ†ä¸ºå“ªäº›å­é—®é¢˜ï¼Ÿå¹¶ä¸”å°†æ¯ä¸ªå°é—®é¢˜ä½œä¸ºä¼˜åŒ–è¿‡çš„é—®é¢˜è¾“å‡ºã€‚
ä¼˜åŒ–è¿‡çš„é—®é¢˜å°†ä»¥â€œä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼ŒåŸºäºŽå·²æœ‰é—®é¢˜%åŽŸæœ‰é—®é¢˜%ï¼Œæˆ‘å°†é—®é¢˜æ‹†åˆ†ä¸ºå‡ ä¸ªå°é—®é¢˜ï¼Œæ‰€ä»¥æƒ³å‘ä½ å’¨è¯¢ä¸‹åˆ—é—®é¢˜ï¼šâ€ï¼Œä»¥â€œè¯·é€ä¸ªå›žç­”ä¸Šè¿°é—®é¢˜ï¼Œä¸€æ­¥æ­¥æ€è€ƒã€‚â€ç»“å°¾ã€‚

ä¾‹å­1ï¼š
è¾“å…¥ï¼šå¦‚ä½•ä½¿ç”¨è¯­è¨€æ¨¡åž‹ï¼Ÿ
ä¿®æ”¹åŽçš„è¾“å…¥ï¼š
ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼ŒåŸºäºŽå·²æœ‰é—®é¢˜â€œå¦‚ä½•ä½¿ç”¨è¯­è¨€æ¨¡åž‹ï¼Ÿâ€ï¼Œæˆ‘å°†é—®é¢˜æ‹†åˆ†ä¸ºå‡ æ­¥ï¼Œæ‰€ä»¥æƒ³å‘ä½ å’¨è¯¢ä¸‹åˆ—é—®é¢˜ï¼š

1.å¦‚ä½•èŽ·å–è¯­è¨€æ¨¡åž‹çš„ä½¿ç”¨èµ„æ ¼ï¼Ÿ
2.å¦‚ä½•å­¦ä¹ è¯­è¨€æ¨¡åž‹çš„ä½¿ç”¨æ–¹æ³•ï¼Ÿ
3.å¦‚ä½•ç†Ÿç»ƒæŽŒæ¡è¯­è¨€æ¨¡åž‹çš„ä½¿ç”¨

è¯·é€ä¸ªå›žç­”ä¸Šè¿°é—®é¢˜ï¼Œä¸€æ­¥æ­¥æ€è€ƒã€‚

ä¾‹å­2ï¼š
è¾“å…¥ï¼šå¦‚ä½•èµšåˆ°100ä¸‡ï¼Ÿ
ä¿®æ”¹åŽçš„è¾“å…¥ï¼š
ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼ŒåŸºäºŽå·²æœ‰é—®é¢˜â€œå¦‚ä½•èµšåˆ°100ä¸‡ï¼Ÿâ€ï¼Œæˆ‘å°†é—®é¢˜æ‹†åˆ†ä¸ºå‡ æ­¥ï¼Œæ‰€ä»¥æƒ³å‘ä½ å’¨è¯¢ä¸‹åˆ—é—®é¢˜ï¼š
1.å¦‚ä½•æ‰¾åˆ°ä¸€ä»½é«˜è–ªçš„å·¥ä½œï¼Ÿ
2. å¦‚ä½•æé«˜è‡ªå·±çš„æŠ€èƒ½å’ŒçŸ¥è¯†æ°´å¹³ä»¥èŽ·å¾—æ›´é«˜è–ªèµ„ï¼Ÿ
3. å¦‚ä½•åˆç†è§„åˆ’è´¢åŠ¡ä»¥å®žçŽ°è´¢å¯Œå¢žå€¼ï¼Ÿ
4. åœ¨ä»€ä¹ˆè¡Œä¸šæˆ–é¢†åŸŸä¸­æ›´å®¹æ˜“èµšå–ç™¾ä¸‡ä»¥ä¸Šçš„æ”¶å…¥ï¼Ÿ
5. å¦‚ä½•åœ¨èŒåœºä¸Šå»ºç«‹è‰¯å¥½çš„äººé™…å…³ç³»ä»Žè€Œæå‡ä¸ªäººä»·å€¼ï¼Ÿ
"""

code_instruction = """
æ‚¨æ˜¯ä¸€ä¸ªä»£ç ä¼˜åŒ–AIåŠ©æ‰‹ã€‚
è¯·ä¿®æ”¹å¹¶ä¼˜åŒ–ä»¥ä¸‹ä»£ç é—®é¢˜ï¼Œä»¥ä½¿å…¶æ›´æ˜Žç¡®å’Œæ¸…æ™°ã€‚è¯·ç¡®ä¿æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œè¯¦ç»†æè¿°ï¼Œä»¥ä¾¿æ›´å¥½åœ°å›žç­”é—®é¢˜ã€‚å¦‚æžœä½ çš„ç­”æ¡ˆåŒ…æ‹¬ä»£ç ï¼Œç”¨markdownæ ¼å¼å±•ç¤ºä»£ç éƒ¨åˆ†ã€‚
"""

judge_instruction = """ä½ æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡åž‹æç¤ºè¯è¯„åˆ†å¤§å¸ˆï¼Œç”¨æˆ·å°†è¾“å…¥ä¸€ä¸ªæç¤ºè¯ï¼Œä½ é’ˆå¯¹è¯¥é—®é¢˜ç»™å‡ºä½ çš„è¯„åˆ†ä»¥åŠä½ çš„å»ºè®®ï¼Œè¯„åˆ†ä¸º0-10ã€‚

è¯„åˆ†ä¾æ®ä¸€ä¸‹å‡ ç‚¹ï¼š
1. è¯¥é—®é¢˜æ˜¯å¦æœ‰é€»è¾‘è°¬è¯¯ã€æˆ–è€…æ˜¯æ­§ä¹‰ã€‚
2. è¯¥é—®é¢˜æ˜¯å¦æœ‰è¯­æ³•é”™è¯¯ã€é”™åˆ«å­—ã€‚
3. è¯¥é—®é¢˜æ˜¯å¦è¶³å¤Ÿå…·ä½“ï¼Œèƒ½å¸®åŠ©è¯­è¨€æ¨¡åž‹ç†è§£ä»–æ‰€è¡¨è¾¾çš„æ„æ€ï¼Œå¤ªç®€çŸ­æˆ–è€…å®½æ³›çš„é—®é¢˜æ˜¯ç³Ÿç³•çš„ã€‚
4. è¯¥é—®é¢˜æ˜¯å¦è¢«æ‹†åˆ†ä¸ºäº†å¤šä¸ªå°é—®é¢˜ï¼Œæ¿€å‘è¯­è¨€æ¨¡åž‹åˆ†æ­¥éª¤æ€è€ƒçš„èƒ½åŠ›ï¼Œä»¥ä¾¿è¯­è¨€æ¨¡åž‹ä¸€æ­¥æ­¥è§£å†³å®ƒã€‚
5. è¯¥é—®é¢˜æ˜¯å¦æä¾›äº†ä¾‹å­ï¼Œä¾¿äºŽè¯­è¨€æ¨¡åž‹ç†è§£å…·ä½“æƒ…æ™¯ã€‚

è¯„åˆ†çš„å°ºåº¦æ˜¯ï¼š
10åˆ†-è¶³å¤Ÿå®Œç¾Žï¼Œå¯ä»¥å‘å¤§è¯­è¨€æ¨¡åž‹æé—®ã€‚
8åˆ†-åŸºæœ¬å®Œç¾Žï¼Œä½†æ²¡æœ‰æä¾›ä¾‹å­æˆ–æ˜¯æ²¡æœ‰æ‹†åˆ†ä¸ºå¤šä¸ªå°é—®é¢˜ã€‚
6åˆ†-æ²¡æœ‰æ˜Žæ˜¾çš„é€»è¾‘ã€è¯­æ³•é”™è¯¯ï¼Œä½†æ˜¯è¿˜æœ‰æ”¹è¿›çš„ç©ºé—´ã€‚
4åˆ†-é—®é¢˜è¾ƒå¤§ã€‚
2åˆ†-éœ€è¦é‡å†™ã€‚

è¯·ä¾èµ–ä½ çš„ä¸“ä¸šçŸ¥è¯†ï¼Œç»™å‡ºä½ çš„è¯„åˆ†ï¼Œè¯·ä¸è¦ç»™å‡ºè¿‡é«˜çš„è¯„åˆ†ï¼Œä½ éœ€è¦ä¸¥æ ¼ä¸€äº›ï¼Œå°†è¯„åˆ†æ”¾åœ¨æ–¹æ‹¬å·ä¸­ï¼Œä¹‹åŽä½ å¯ä»¥åŠ ä¸Šå»ºè®®ã€‚ä¾‹å¦‚ï¼š[8.5]ï¼Œè¯„åˆ†èŒƒå›´ä¸º0-10åˆ†ï¼Œå°æ•°ç‚¹åŽä¿ç•™ä¸€ä½å°æ•°ã€‚
"""

default_instruction = """æ‚¨æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡åž‹æç¤ºè¯æ’°å†™ä¸“å®¶ï¼Œä½ çš„å·¥ä½œæ˜¯ä¸ºç”¨æˆ·ä¼˜åŒ–ä»–ä»¬çš„é—®é¢˜ï¼Œè¾“å‡ºæ›´å¥½çš„é—®é¢˜ï¼Œè€Œéžå›žç­”é—®é¢˜ï¼Œä½ ä¿®æ”¹åŽçš„ç­”æ¡ˆå°†æä¾›ç»™æ›´å¼ºå¤§çš„è¯­è¨€æ¨¡åž‹ï¼Œç”±ä»–ä»¬æ¥ç»™å‡ºé—®é¢˜çš„ç­”æ¡ˆã€‚
æŽ¥ä¸‹æ¥ï¼Œç”¨æˆ·ä¼šå‘ä½ æä¾›ä¸€ä¸ªé—®é¢˜ï¼Œä½ éœ€è¦å°†é—®é¢˜ä¿®æ”¹ä¸ºæ›´å¥½çš„é—®é¢˜è¾“å‡ºï¼Œè¯·æ³¨æ„è¾“å‡ºçš„é—®é¢˜ä¸é’ˆå¯¹ç”¨æˆ·ï¼Œä¸æ˜¯å¯¹äºŽç”¨æˆ·éœ€æ±‚çš„å†åº¦ç¡®è®¤ï¼Œè€Œç«™åœ¨ç”¨æˆ·è§†è§’å¯ä»¥ç”¨äºŽå‘æ›´å¼ºå¤§çš„è¯­è¨€æ¨¡åž‹æé—®ã€‚
æ¢å¥è¯è¯´ï¼Œä½ è¦æ›¿ç”¨æˆ·é—®å‡ºæ›´å¥½çš„é—®é¢˜ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œä½ éœ€è¦è®©é—®é¢˜æ›´åŠ æ˜Žç¡®ã€æœ€å¥½èƒ½æä¾›ä¾‹å­è¯´æ˜Žã€æ”¹æ­£é—®é¢˜ä¸­çš„è¯­æ³•é”™è¯¯ä»¥åŠäº‹å®žæ€§é”™è¯¯ã€‚

ä½ çš„å›žç­”å°†ä»¥â€ä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼Œæˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼šâ€œå¼€å¤´ã€‚è¯·ä»¥é—®å·ä½œä¸ºä½ è¾“å‡ºçš„ç»“å°¾ã€‚

ä¾‹å­1ï¼š
é—®é¢˜è¾“å…¥ï¼šå¤æ—¦ å“²å­¦ç³»
é—®é¢˜è¾“å‡ºï¼šä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼æˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼šå¤æ—¦å“²å­¦ç³»ä¸“ä¸šå®žåŠ›æ€Žä¹ˆæ ·ï¼Ÿè¯·ä¸ºæˆ‘ä»‹ç»å…¶æ¦‚æ‹¬ã€‚

ä¾‹å­2ï¼š
é—®é¢˜è¾“å…¥ï¼šé•¿åŸŽå†å“ªé‡Œ
é—®é¢˜è¾“å‡ºï¼šä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼æˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼šé•¿åŸŽåœ¨å“ªé‡Œ

ä¾‹å­3ï¼š
é—®é¢˜è¾“å…¥ï¼šcontainerå¤æ•°
é—®é¢˜è¾“å‡ºä½ å¥½ï¼ŒAIåŠ©æ‰‹ï¼æˆ‘æƒ³å‘ä½ å’¨è¯¢ä¸€ä¸‹é—®é¢˜ï¼šè‹±è¯­å•è¯containerçš„å¤æ•°å½¢å¼æ˜¯ä»€ä¹ˆ
"""


def extract_transformed_text(response: str) -> str:
    """
    Extracts the transformed text from the response.

    Args:
        response (str): The response from the language model.

    Returns:
        str: The extracted transformed text.
    """
    # Use regex to extract text after "ä¿®æ”¹çš„è¾“å…¥å¦‚ä¸‹ï¼š"
    match = re.search(r'ä¿®æ”¹çš„è¾“å…¥å¦‚ä¸‹ï¼š(.*)', response, re.DOTALL)
    if match:
        transformed_text = match.group(1).strip()
        return transformed_text
    else:
        return "No modified question found"


logging.info("User input begins")
if user_input := st.chat_input("What is up?"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    with st.chat_message("assistant"):
        with st.spinner("Wait for it..."):
            logging.info("Calling chains")
            if (user_input_type == "default"):
                _ = default_instruction
                response = chain.predict(
                    type_instruction=_, user_input=user_input)
            elif (user_input_type == "code"):
                _ = code_instruction
                response = chain.predict(
                    type_instruction=_, user_input=user_input)
            elif (user_input_type == "judge"):
                _ = judge_instruction
                response = chain.predict(
                    type_instruction=_, user_input=user_input)
            elif (user_input_type == "multi-step"):
                _ = multi_step_instruction
                response = multi_step_chain.predict(
                    type_instruction=_, user_input=user_input)
            else:
                st.warning("Please select an input type", icon = "ðŸš¨")
        extracted_response = extract_transformed_text(response)
        logging.info("extracted_response: " + extracted_response)
        response_newlines = extracted_response.replace('\n', '\n\n')
        st.markdown(response_newlines)
    st.session_state.messages.append(
        {"role": "assistant", "content": response_newlines})
